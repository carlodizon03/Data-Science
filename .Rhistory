ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
set.seed(123)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- dt[which(dt$category_id == "Comedy" | dt$category_id == "Music"  | dt$category_id == "Sports"),]
dt <- sample_n(dt,100)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
library(party)
library(randomForest)
output.forest <- randomForest(formula = category_id ~. , data = ds.train ,importance = TRUE, ntree = 100)
output.forest
plot(output.forest)
predTrain <- predict(output.forest, ds.train, type = "class")
table(predTrain, ds.train$category_id)
predTest <- predict(output.forest, ds.test , type = "class")
mean(predTest == ds.test$category_id)
table(predTest, ds.test$category_id)
print(importance(output.forest,type = 2))
varImpPlot(output.forest)
library("C50")
DT <- C5.0(formula = category_id ~., data =   ds.train)
summary(DT)
plot(DT)
result <- predict(DT,ds.test[1:5,trainx.vars])
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.traint$category_id)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
predTrain <- predict(output.forest, ds.train, type = "class")
table(predTrain, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
set.seed(123)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- dt[which(dt$category_id == "Comedy" | dt$category_id == "Music"  | dt$category_id == "Sports"),]
dt <- sample_n(dt,100)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
library(party)
library(randomForest)
output.forest <- randomForest(formula = category_id ~. , data = ds.train ,importance = TRUE, ntree = 100)
output.forest
plot(output.forest)
predTrain <- predict(output.forest, ds.train, type = "class")
table(predTrain, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
predTest <- predict(output.forest, ds.test , type = "class")
mean(predTest == ds.test$category_id)
table(predTest, ds.test$category_id)
print(importance(output.forest,type = 2))
varImpPlot(output.forest)
library("C50")
DT <- C5.0(formula = category_id ~., data =   ds.train)
summary(DT)
plot(DT)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
DT.test_res = predict(DT, newdata = ds.test)
table(DT.test_res, ds.test$category_id)
mean(DT.test_res == ds.test$category_id)
RF.train_res <- predict(RF, ds.train, type = "class")
set.seed(123)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- dt[which(dt$category_id == "Comedy" | dt$category_id == "Music"  | dt$category_id == "Sports"),]
dt <- sample_n(dt,100)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
library(party)
library(randomForest)
RF <- randomForest(formula = category_id ~. , data = ds.train ,importance = TRUE, ntree = 100)
RF
plot(RF)
RF.train_res <- predict(RF, ds.train, type = "class")
table(RF.train_res , ds.train$category_id)
mean(RF.train_res  == ds.train$category_id)
predTest <- predict(output.forest, ds.test , type = "class")
mean(predTest == ds.test$category_id)
table(predTest, ds.test$category_id)
print(importance(output.forest,type = 2))
varImpPlot(output.forest)
library("C50")
DT <- C5.0(formula = category_id ~., data =   ds.train)
summary(DT)
plot(DT)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
DT.test_res = predict(DT, newdata = ds.test)
table(DT.test_res, ds.test$category_id)
mean(DT.test_res == ds.test$category_id)
library(e1071)
NB = naiveBayes(formula = category_id ~., data = ds.train)
summary(NB)
library(e1071)
NB = naiveBayes(formula = category_id ~., data = ds.train)
summary(NB)
plot(NB)
library(e1071)
NB = naiveBayes(formula = category_id ~., data = ds.train)
summary(NB)
NB.train_res = predict(NB, newdata = ds.train)
table(NB.train_res, ds.test$category_id)
NB.train_res = predict(NB, newdata = ds.train)
table(NB.train_res, ds.train$category_id)
set.seed(123)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- dt[which(dt$category_id == "Comedy" | dt$category_id == "Music"  | dt$category_id == "Sports"),]
dt <- sample_n(dt,100)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
library(party)
library(randomForest)
RF <- randomForest(formula = category_id ~. , data = ds.train ,importance = TRUE, ntree = 100)
RF
plot(RF)
RF.train_res <- predict(RF, ds.train, type = "class")
table(RF.train_res , ds.train$category_id)
mean(RF.train_res  == ds.train$category_id)
predTest <- predict(output.forest, ds.test , type = "class")
mean(predTest == ds.test$category_id)
table(predTest, ds.test$category_id)
print(importance(output.forest,type = 2))
varImpPlot(output.forest)
library("C50")
DT <- C5.0(formula = category_id ~., data =   ds.train)
summary(DT)
plot(DT)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
DT.test_res = predict(DT, newdata = ds.test)
table(DT.test_res, ds.test$category_id)
mean(DT.test_res == ds.test$category_id)
library(e1071)
NB = naiveBayes(formula = category_id ~., data = ds.train)
summary(NB)
NB.train_res = predict(NB, newdata = ds.train)
table(NB.train_res, ds.train$category_id)
mean(NB.train_res == ds.train$category_id)
NB.test_res = predict(NB, newdata = ds.test)
table(NB.test_res, ds.test$category_id)
mean(NB.test_res == ds.test$category_id)
set.seed(123)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- dt[which(dt$category_id == "Comedy" | dt$category_id == "Music"  | dt$category_id == "Sports"),]
dt <- sample_n(dt,100)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.train <-ds[1:70,c("views","likes","dislikes","comment_count","category_id")]
ds.test <- ds[71:100,c("views","likes","dislikes","comment_count","category_id")]
ds.train$category_id <- droplevels(ds.train$category_id)
ds.test$category_id <- droplevels(ds.test$category_id)
prop.table(table(ds.train$category_id))
prop.table(table(ds.test$category_id))
library(party)
library(randomForest)
RF <- randomForest(formula = category_id ~. , data = ds.train ,importance = TRUE, ntree = 100)
RF
plot(RF)
RF.train_res <- predict(RF, ds.train, type = "class")
table(RF.train_res , ds.train$category_id)
mean(RF.train_res  == ds.train$category_id)
predTest <- predict(output.forest, ds.test , type = "class")
mean(predTest == ds.test$category_id)
table(predTest, ds.test$category_id)
print(importance(output.forest,type = 2))
varImpPlot(output.forest)
library("C50")
DT <- C5.0(formula = category_id ~., data =   ds.train)
summary(DT)
plot(DT)
DT.train_res = predict(DT, newdata = ds.train )
table(DT.train_res, ds.train$category_id)
mean(DT.train_res == ds.train$category_id)
DT.test_res = predict(DT, newdata = ds.test)
table(DT.test_res, ds.test$category_id)
mean(DT.test_res == ds.test$category_id)
library(e1071)
NB = naiveBayes(formula = category_id ~., data = ds.train)
summary(NB)
NB.train_res = predict(NB, newdata = ds.train)
table(NB.train_res, ds.train$category_id)
mean(NB.train_res == ds.train$category_id)
NB.test_res = predict(NB, newdata = ds.test)
table(NB.test_res, ds.test$category_id)
mean(NB.test_res == ds.test$category_id)
RF.test_res <- predict(RF, ds.test , type = "class")
mean(RF.test_res == ds.test$category_id)
table(RF.test_res, ds.test$category_id)
set.seed(123)
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
dt <- read.csv("Data\\USvideos_catNames.csv")
set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {

set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
plot(data,col=result$clustering)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
result
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
result$clustering
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
result$clustering
plot(ds.toGowerCluster,result$clustering)
View(ds.toGowerCluster)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
result = pam(mat, 3)
hier = hclust(mat)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
hier = hclust(mat)
plot(hier)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
hier = hclust(mat)
plot(hier)
res = cutree(hier,3)
plot(ds.toGowerCluster,col=res)
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
mat
hier = hclust(mat)
plot(hier)
res = cutree(hier,3)
plot(ds.toGowerCluster,col=res)
install.packages(fpc)
install.packages("fpc")
library(fpc)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 1)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = .1)
plot(ds.toGowerCluster, col=res$cluster)â€
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = .1)
plot(ds.toGowerCluster, col=res$cluster)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = .01)
plot(ds.toGowerCluster, col=res$cluster)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 1)
plot(ds.toGowerCluster, col=res$cluster)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 2)
plot(ds.toGowerCluster, col=res$cluster)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 1)
plot(ds.toGowerCluster, col=res$cluster)
set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
hier = hclust(mat)
plot(hier)
res = cutree(hier,3)
plot(ds.toGowerCluster,col=res)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 1)
plot(ds.toGowerCluster, col=res$cluster)
set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos_catNames.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
hier = hclust(mat)
plot(hier)
res = cutree(hier,3)
plot(ds.toGowerCluster,col=res)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 3)
plot(ds.toGowerCluster, col=res$cluster)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 3)
res
plot(ds.toGowerCluster, col=res$cluster)
set.seed(12)
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library(cluster)
mat = daisy(ds.toGowerCluster, metric = "gower")
hier = hclust(mat)
plot(hier)
res = cutree(hier,3)
plot(ds.toGowerCluster,col=res)
library(fpc)
mat = daisy(ds.toGowerCluster, metric = "gower")
res = dbscan(mat,eps = 3)
plot(ds.toGowerCluster, col=res$cluster)
View(ds)
View(ds)
