#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print("Views "+entropy(ds.toGowerCluster$views))
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print("Views", entropy(ds.toGowerCluster$views))
require(stats)
ts(1:20)  #-- print is the "Default function" --> print.ts(.) is called
for(i in 1:3) print(1:i)
## Printing of factors
attenu$station ## 117 levels -> 'max.levels' depending on width
## ordered factors: levels  "l1 < l2 < .."
esoph$agegp[1:12]
esoph$alcgp[1:12]
## Printing of sparse (contingency) tables
set.seed(521)
t1 <- round(abs(rt(200, df = 1.8)))
t2 <- round(abs(rt(200, df = 1.4)))
table(t1, t2) # simple
print(table(t1, t2), zero.print = ".") # nicer to read
## same for non-integer "table":
T <- table(t2,t1)
T <- T * (1+round(rlnorm(length(T)))/4)
print(T, zero.print = ".") # quite nicer,
print.table(T[,2:8] * 1e9, digits=3, zero.print = ".")
## still slightly inferior to  Matrix::Matrix(T)  for larger T
## Corner cases with empty extents:
table(1, NA) # < table of extent 1 x 0 >
print(paste0("Current working dir: ", wd))
wd = 123
print(paste0("Current working dir: ", wd))
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("Views: ",entropy(ds.toGowerCluster$views))
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("Views: ",entropy(ds.toGowerCluster$views)))
View(ds)
View(ds)
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
ds[pam_fit$medoids,]
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam_fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam_results$the_summary
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
ds[pam_fit$medoids,]
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam_fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam_results$the_summary
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
library(factoextra)
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
ds[pam_fit$medoids,]
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam_fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam_results$the_summary
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
library(factoextra)
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
ds[pam_fit$medoids,]
library(cluster)
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam_fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam_results$the_summary
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
library(factoextra)
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
ds[pam_fit$medoids,]
library(cluster)
pam.fit <- pam(gower_dist, diss = TRUE, k = 3)
pam.res <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam.fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam.res$the_summary
ds[pm$medoids,]
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
library(factoextra)
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
library(cluster)
pam.fit <- pam(gower_dist, diss = TRUE, k = 3)
pam.res <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam.fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam.res$the_summary
ds[pam_fit$medoids,]
library("dplyr")
dt <- read.csv("Data\\USvideos.csv")
dt <- sample_n(dt,50)
ds <- dt[,c("video_id","views","likes","dislikes","comment_count","category_id","trending_date")]
ds <- na.omit(ds)
normalized<-function(y) {
x<-y[!is.na(y)]
x<-(x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
ds$views <- normalized(ds$views)
ds$likes <- normalized(ds$likes)
ds$dislikes <- normalized(ds$dislikes)
ds$comment_count <- normalized(ds$comment_count)
ds.toGowerCluster <-  ds[,c("views","likes","dislikes","comment_count","category_id")]
#compute Shannon entropy
entropy <- function(target) {
freq <- table(target)/length(target)
# vectorize
vec <- as.data.frame(freq)[,2]
#drop 0 to avoid NaN resulting from log2
vec<-vec[vec>0]
#compute entropy
-sum(vec * log2(vec))
}
print(paste0("video_id Entropy: ",entropy(ds$video_id)))
print(paste0("Views Entropy: ",entropy(ds$views)))
print(paste0("likes Entropy: ",entropy(ds$likes)))
print(paste0("dislikes Entropy: ",entropy(ds$dislikes)))
print(paste0("comment_count Entropy: ",entropy(ds$comment_count)))
print(paste0("category_id Entropy: ",entropy(ds$category_id)))
print(paste0("trending_date Entropy: ",entropy(ds$trending_date)))
library("cluster")
gower_dist <- daisy(ds.toGowerCluster, metric="gower")
distance_matrix <- as.matrix(gower_dist)
# Output most similar pair
ds[which(distance_matrix == min(distance_matrix[distance_matrix != min(distance_matrix)]),arr.ind = TRUE)[1, ], ]
# Output most dissimilar pair
ds[
which(distance_matrix == max(distance_matrix[distance_matrix != max(distance_matrix)]),
arr.ind = TRUE)[1, ], ]
library(factoextra)
fviz_nbclust(ds.toGowerCluster, pam, method="silhouette")+theme_classic()
fviz_nbclust(ds.toGowerCluster, pam, method="wss")+theme_classic()
pm <- eclust(distance_matrix,FUNcluster="pam", k=3)
fviz_cluster(pm,distance_matrix)
print(pm)
library(cluster)
pam.fit <- pam(gower_dist, diss = TRUE, k = 3)
pam.res <- ds %>%
dplyr::select(-"video_id") %>%
mutate(cluster = pam.fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam.res$the_summary
ds[pam.fit$medoids,]
R: tinytex::install_tinytex()
tinytex::install_tinytex()
